algorithm: SSVEPformerX_CVFusion

# Parameters for training procedure
train_param:
  UD: 0                  # -1——Unsupervised, 0——User-Dependent；1——User-Independent
  ratio: 3               # -1——Training-Free, 1——80% vs 20%;2——50% vs 50%;3——20% vs 80%(UD Approach)
  seeds: [2021, 2022, 2023, 2024, 2025]            # 0 or else——(N-1)/N vs 1/N(UI Approach)
  # seed: 2025
# Parameters for ssvep data
data_param:
  ws: 1.0                     # window size of ssvep
  Nh: 32                      # number of trial
  Nc: 8                        # number of channel
  Fs: 250                      # frequency of sample
  Nf: 8                       # number of stimulus
  Ns: 15                      # number of subjects
  dual: true                  #  use time domain data
                      # number of subjects
# Parameters for ssvep BETA data
data_param_BETA:
  ws: 2.0                      # window size of ssvep
  Nh: 32                      # number of trial
  Nc: 8                        # number of channel
  Fs: 250                      # frequency of sample
  Nf: 8                       # number of stimulus
  Ns: 15                      # number of subjects


# Parameters for DL-based methods
EEGNet:
  epochs: 500                  # number of epochs
  bz: 30                     # batch size
  lr: 0.001                       # learning rate
  wd: 0.0001                   # weight decay
  lr_jitter: false             # learning rate scheduler

CCNN:
  epochs: 500                  # number of epochs
  bz: 30                     # batch size
  lr: 0.001                       # learning rate
  wd: 0.0001                   # weight decay
  lr_jitter: false             # learning rate scheduler

FBtCNN:
  epochs: 300                  # number of epochs
  bz: 30                     # batch size
  lr: 0.001                       # learning rate
  wd: 0.01                   # weight decay
  lr_jitter: false             # learning rate scheduler

ConvCA:
  epochs: 1000                  # number of epochs
  bz: 30                     # batch size
  lr: 0.0008                       # learning rate
  wd: 0.0000                  # weight decay
  lr_jitter: false             # learning rate scheduler

SSVEPNet:
  epochs: 500                  # number of epochs
  bz: 30                     # batch size
  lr: 0.01                       # learning rate
  wd: 0.0003                   # weight decay
  lr_jitter: true             # learning rate scheduler
  stimulus_type: 12            # 4-class or 12-class

SSVEPformer:
  epochs: 300                 # number of epochs
  bz: 30                     # batch size
  lr: 0.001                       # learning rate
  wd: 0.0001                   # weight decay
  lr_jitter: false             # learning rate scheduler

SSVEPformerX:
  epochs: 300           # 训练轮数（略高于SSVEPformer以确保收敛）
  bz: 30                # batch size（保持一致）
  lr: 0.001             # 学习率，与SSVEPformer一致
  wd: 0.0001            # 权重衰减
  lr_jitter: false      # 不启用学习率抖动（可后期打开）
  dropout: 0.3          # 中等dropout，更适合子带与交叉注意
  depth_freq: 1         # Transformer层数（轻量）
  depth_time: 2
  attention_kernel_length: 7   # 卷积注意核长度，适中
  use_subband:  true      # 开启子带建模模块
  subband_proj: dwconv3   # linear | identity | dwconv3
  use_crossview: true      # 开启双视图交叉注意模块
  crossview_alpha: 0.5       # 残差缩放，消融用 0.2
  use_harmonic_pe: true    # 开启谐波位置编码模块
  harmonic_beta: 0.5        # 强度缩放，消融用 0.1
  pe_mode: fixed          # harmonic | fixed  (可选)
  optimizer: SGD

SSVEPformerX_CVFusion:
  epochs: 300           # 训练轮数（略高于SSVEPformer以确保收敛）
  bz: 30                # batch size（保持一致）
  lr: 0.001             # 学习率，与SSVEPformer一致
  wd: 0.0001            # 权重衰减
  lr_jitter: false      # 不启用学习率抖动（可后期打开）
  dropout: 0.3          # 中等dropout，更适合子带与交叉注意
  depth: 3
  attention_kernal_length: 31
  token_dim: 560        # 频域长度（与频谱处理一致）

  use_subband: true
  subband_proj: "linear"
  use_crossview: true
  crossview_alpha: 1.0
  use_harmonic_pe: true
  harmonic_beta: 1.0
  pe_mode: "harmonic"

  enable_time_branch: true
  time_token_dim: 250   # 时域长度（Nt = Fs*ws）
  resize_time_to_freq: true

  use_len_fusion_block: true
  fusion_mode: "chan_cat"

DDGCNN:
  epochs: 500                  # number of epochs
  bz: 30                     # batch size
  lr: 0.001                   # learning rate
  wd: 0.0001                   # weight decay
  lr_jitter: true             # learning rate scheduler
  lr_decay_rate: 0.75         # learning rate decay rate
  optim_patience: 300        # optimizer patience
  trans_class: DCD           # {DCD, linear, normal_conv}
  act: leakyrelu             # activation layer {relu, prelu, leakyrelu}
  norm: layer                # {batch, layer, instance} normalization
  n_filters: 128            # 64 or 128






